{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/siddharthpandey/Desktop/MS/DIC/Lab2/data/cc\n",
      "*******END******\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import warc\n",
    "from itertools import islice\n",
    "from langdetect import detect\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "\n",
    "urllist = []\n",
    "import warc\n",
    "print(os.getcwd())\n",
    "f = warc.open(os.getcwd()+'/'+\"CC-MAIN-20190323020538-20190323042538-00124.warc.gz\")\n",
    "for record in f:\n",
    "    #print record['WARC-Target-URI'], record['Content-Length']\n",
    "    url = record.header.get('warc-target-uri', None)  \n",
    "    if not url:\n",
    "        continue\n",
    "    else:\n",
    "        urllist.append(url)\n",
    "        #print(url)\n",
    "#print(len(urllist))\n",
    "urllist = list(dict.fromkeys(urllist))\n",
    "topics = [\"cybersecurity\",\"hack\",\"cybercrime\",\"darkweb\",\"hacker\",\"cyber\",\"infosec\",\"tech\",\"phishing\",\"blackhat\"]\n",
    "urlcount=0\n",
    "cleanlist = []\n",
    "for i in urllist:\n",
    "    urlcount = urlcount+1\n",
    "    count=0\n",
    "    #print(i)\n",
    "    #print(urlcount)\n",
    "    #if(urlcount == 20):\n",
    "        #print(\"SLEEPING FOR 5 SECONDS\")\n",
    "        #time.sleep(5)\n",
    "    urlcount=0\n",
    "    for topic in topics:\n",
    "        if(i.find(topic)!=-1):\n",
    "            print(\"found for \"+topic)\n",
    "            print(i)\n",
    "            cleanlist.append(i)\n",
    "\n",
    "for link in cleanlist:\n",
    "    count=0;\n",
    "    try:\n",
    "        response=requests.get(link)\n",
    "    except:\n",
    "        continue\n",
    "    stripped = re.sub('<[^<]+?>', '', response.text)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    res=\"\"\n",
    "    for j in soup.findAll('p'):\n",
    "        res=res+str(j)\n",
    "    content = remove_tags(res)\n",
    "    #print(content)\n",
    "    try:\n",
    "        language = detect(content.decode('utf-8'))\n",
    "        print(language)\n",
    "    except:\n",
    "        print(\"continuing...\")\n",
    "        continue\n",
    "    #print(content)\n",
    "    if(language =='en'):\n",
    "        for topic in topics:\n",
    "            if(content.find(topic)):\n",
    "                count = count+1\n",
    "        if(count>0):\n",
    "            print(\"***** DATA FOUND FOR : \"+topic+\" ********\")\n",
    "            finalString = \"\"\n",
    "            punctuations = '''!()-[]{};:'’\"\\,.,”“.“‘<>—./?@#$%^&*_~'''\n",
    "            for character in content:\n",
    "                if(character not in punctuations):\n",
    "                    finalString = finalString + character\n",
    "            stop_words = set(stopwords.words('english')) \n",
    "            word_tokens = word_tokenize(finalString)\n",
    "            filtered_sentence = []\n",
    "            for w in word_tokens: \n",
    "                if w not in stop_words: \n",
    "                    filtered_sentence.append(unicode(w, errors='ignore'))\n",
    "            finalStem =[]\n",
    "            porter = PorterStemmer()\n",
    "            for words in range(0,len(filtered_sentence)):\n",
    "                wrd = porter.stem(filtered_sentence[words])\n",
    "                finalStem.append(wrd)\n",
    "            print(\"Entering data in file...\")\n",
    "            file1 = open(os.getcwd()+'/rawdata/'+'commoncrawlNEW12'+'.txt', 'a')\n",
    "            for data in (finalStem):\n",
    "                file1.write(data.encode('utf-8')+' ')\n",
    "            file1.write('\\n\\n')\n",
    "            file1.close()\n",
    "            print(\"*********** FILE WRITE COMPLETE ***************\") \n",
    "        else:\n",
    "            continue\n",
    "print(\"*******END******\")\n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
